<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>技术实践 on 指尖魔法屋</title>
    <link>https://blog.thinkmoon.cn/categories/%E6%8A%80%E6%9C%AF%E5%AE%9E%E8%B7%B5/</link>
    <description>Recent content in 技术实践 on 指尖魔法屋</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>© 2025 ThinkBlog. All rights reserved.</copyright>
    <lastBuildDate>Tue, 16 Dec 2025 19:40:00 +0000</lastBuildDate>
    <atom:link href="https://blog.thinkmoon.cn/categories/%E6%8A%80%E6%9C%AF%E5%AE%9E%E8%B7%B5/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>从零开始实现长语音转文本：从理论到实践</title>
      <link>https://blog.thinkmoon.cn/post/990-%E4%BD%BF%E7%94%A8%E6%99%BA%E8%B0%B1api%E5%B0%86%E9%95%BF%E8%AF%AD%E9%9F%B3%E8%BD%AC%E6%96%87%E6%9C%AC/</link>
      <pubDate>Tue, 16 Dec 2025 19:40:00 +0000</pubDate>
      <guid>https://blog.thinkmoon.cn/post/990-%E4%BD%BF%E7%94%A8%E6%99%BA%E8%B0%B1api%E5%B0%86%E9%95%BF%E8%AF%AD%E9%9F%B3%E8%BD%AC%E6%96%87%E6%9C%AC/</guid>
      <description>智谱AI语音识别API在准确性和响应速度方面表现出色，但受限于单次请求最大30秒的音频长度限制。本文基于pyannote.audio的语音活动检测(VAD)技术，实现了智能音频分块策略：在语音段落间隙处精确切割，避免截断语义单元；通过批量异步调用API接口，实现对长音频文件的完整转写；最终拼接处理结果输出。从Conda环境配置到生产代码实现，提供完整的可复现解决方案。&#xA;一、长语音转文本的核心挑战 主要的挑战是目前的条件没有合适的API。正好智谱API有token和API，先拿来用了。但是智谱API只能支持25M以下以及30s以内的语音。所以我们需要语音文件先断句，再分块，接着调用智谱API，最后拼接成最终结果。&#xA;三、关键技术：实现智能音频分块 这是自建系统的核心技术，也是本实践的重点。目标是在静音处切分音频，避免截断单词。&#xA;3.1 核心思想 加载音频：读取长音频文件。 检测静音：遍历音频，识别出静音段（声音能量低于阈值）。 标记边界：以静音为边界，切分语音。 生成音频块：保存为独立的短音频文件。 添加保护垫：在切分点两边保留一小段音频作为缓冲，防止截断。 3.2 三种实现方案 方案 优点 缺点 推荐场景 pydub 极其简单，代码少 精度一般，噪音敏感 快速原型验证、干净录音 webrtcvad 精度高，轻量快速 API底层，代码复杂 对准确率有要求、生产环境 pyannote.audio 效果最好，功能强大（含说话人分离） 依赖重，资源消耗大 需要说话人分离、追求最佳效果 3.3 使用 pyannote.audio 进行分块（本实践选择） pyannote.audio 是一个基于 PyTorch 的开源神经语音工具包，效果最好，且能同时完成说话人分离。选型原则：条件允许范围内，选最好！&#xA;但是本实践没有进行说话人分离&#xA;四、完整实践：使用pyannote.audio切割并调用API转写 实现一个完整的流程：使用 pyannote.audio 切割音频，然后调用智谱AI的API进行转写。&#xA;4.1 环境准备 首先，为了避免系统环境冲突，强烈建议创建一个虚拟环境。&#xA;安装依赖 依赖文件&#xA;name: audio_env channels: - conda-forge - defaults - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ dependencies: - _libgcc_mutex=0.1 - _openmp_mutex=5.1 - aom=3.9.1 - bzip2=1.</description>
    </item>
    <item>
      <title>一些在Jetson设备的开发经验</title>
      <link>https://blog.thinkmoon.cn/post/989_%E4%B8%80%E4%BA%9B%E5%9C%A8jetson%E8%AE%BE%E5%A4%87%E7%9A%84%E5%BC%80%E5%8F%91%E7%BB%8F%E9%AA%8C/</link>
      <pubDate>Mon, 27 Oct 2025 10:30:00 +0000</pubDate>
      <guid>https://blog.thinkmoon.cn/post/989_%E4%B8%80%E4%BA%9B%E5%9C%A8jetson%E8%AE%BE%E5%A4%87%E7%9A%84%E5%BC%80%E5%8F%91%E7%BB%8F%E9%AA%8C/</guid>
      <description>一些在Jetson设备的开发经验 NVIDIA Jetson系列设备作为边缘AI计算的重要平台，在计算机视觉和深度学习应用中发挥着关键作用。本文将详细介绍Jetson设备的刷机、SDK安装、YOLO模型部署以及TensorRT优化等开发经验，帮助开发者快速上手并实现高性能的AI应用部署。&#xA;一、Jetson设备概述 1.1 什么是NVIDIA Jetson？ NVIDIA Jetson是一系列基于ARM架构的嵌入式AI计算平台，专为边缘计算和机器人应用设计。Jetson设备集成了高性能的GPU、CPU和深度学习加速器，能够在低功耗环境下运行复杂的AI模型。&#xA;1.2 Jetson系列对比 设备型号 GPU CPU 内存 功耗 适用场景 Jetson AGX Orin (64GB) 2048-core Ampere 12-core Arm Cortex-A78AE 64GB LPDDR5 60W 高性能AI推理 Jetson Orin NX 16GB 1024-core Ampere 8-core Arm Cortex-A78AE 16GB LPDDR5 25W 中等性能应用 Jetson Orin Nano 1024-core Ampere 6-core Arm Cortex-A78AE 8GB LPDDR5 15W 入门级AI应用 二、JetPack SDK安装与配置 2.1 什么是NVIDIA JetPack？ JetPack是NVIDIA为Jetson设备提供的软件开发套件，包含了完整的操作系统、CUDA、cuDNN、TensorRT等深度学习框架和工具链。&#xA;2.2 刷机步骤 2.2.1 准备工作 下载JetPack SDK Manager 准备USB线缆和电源适配器 2.2.2 刷机流程 下载JetPack SDK Manager</description>
    </item>
    <item>
      <title>x2ray端口转发-解决端口被强</title>
      <link>https://blog.thinkmoon.cn/post/987_x2ray%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91-%E8%A7%A3%E5%86%B3%E7%AB%AF%E5%8F%A3%E8%A2%AB%E5%BC%BA/</link>
      <pubDate>Sat, 10 May 2025 14:19:00 +0000</pubDate>
      <guid>https://blog.thinkmoon.cn/post/987_x2ray%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91-%E8%A7%A3%E5%86%B3%E7%AB%AF%E5%8F%A3%E8%A2%AB%E5%BC%BA/</guid>
      <description>x2ray端口转发-解决端口被强 执行命令，将40000:60000的流量转发到本地443端口&#xA;iptables -t nat -A PREROUTING -p tcp --dport 40000:60000 -j REDIRECT --to-ports 443 </description>
    </item>
    <item>
      <title>Ta-Lib最轻松的安装方式</title>
      <link>https://blog.thinkmoon.cn/post/986_ta-lib%E6%9C%80%E8%BD%BB%E6%9D%BE%E7%9A%84%E5%AE%89%E8%A3%85%E6%96%B9%E5%BC%8F/</link>
      <pubDate>Tue, 15 Apr 2025 14:19:00 +0000</pubDate>
      <guid>https://blog.thinkmoon.cn/post/986_ta-lib%E6%9C%80%E8%BD%BB%E6%9D%BE%E7%9A%84%E5%AE%89%E8%A3%85%E6%96%B9%E5%BC%8F/</guid>
      <description>Ta-Lib最轻松的安装方式 安装conda 初始化环境conda init 执行conda install -c conda-forge ta-lib </description>
    </item>
    <item>
      <title>使用CloudFlare反向代理七牛云存储实现SSL自由</title>
      <link>https://blog.thinkmoon.cn/post/985_%E4%BD%BF%E7%94%A8cloudflare%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E4%B8%83%E7%89%9B%E4%BA%91%E5%AD%98%E5%82%A8%E5%AE%9E%E7%8E%B0ssl%E8%87%AA%E7%94%B1/</link>
      <pubDate>Wed, 19 Mar 2025 14:19:00 +0000</pubDate>
      <guid>https://blog.thinkmoon.cn/post/985_%E4%BD%BF%E7%94%A8cloudflare%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E4%B8%83%E7%89%9B%E4%BA%91%E5%AD%98%E5%82%A8%E5%AE%9E%E7%8E%B0ssl%E8%87%AA%E7%94%B1/</guid>
      <description>基于CloudFlare Workers的七牛云HTTPS代理方案 一、方案背景与核心价值 1.1 问题场景 七牛云存储作为国内主流对象存储服务，其HTTP直连访问存在两大痛点：&#xA;SSL证书成本：自定义域名HTTPS服务需单独购买证书 混合内容风险：主站HTTPS页面加载HTTP资源触发安全警告 1.2 技术选型 通过CloudFlare Workers实现四大核心能力：&#xA;协议转换层：将用户HTTPS请求转换为对七牛HTTP源站的请求 动态内容改写：实时替换响应中的HTTP资源链接 全局缓存加速：利用全球CDN节点提升访问速度 零运维成本：无需维护服务器基础设施 二、原理说明 Cloudflare Worker 是一个无服务器计算平台，通过拦截请求并修改响应实现代理。核心思路是：&#xA;将用户 HTTPS 请求转发到原始 HTTP 站点。 修改响应头，强制 HTTPS 协议并修复混合内容问题。&#xA;三、操作步骤 1. 创建 Cloudflare Worker 登录 Cloudflare 控制台 → Workers &amp;amp; Pages → 创建新 Worker。 进入代码编辑界面。 2. 编写代理脚本 addEventListener(&amp;#39;fetch&amp;#39;, event =&amp;gt; { event.respondWith(handleRequest(event.request)); }); async function handleRequest(request) { // 替换为你的 HTTP 源站地址 const originUrl = &amp;#39;http://your-http-site.com&amp;#39;; const url = new URL(request.url); // 构建指向 HTTP 源站的新请求 const newRequest = new Request(originUrl + url.</description>
    </item>
  </channel>
</rss>
